# Neural Program Synthesis from Diverse Demonstration Videos

## Descriptions
This project is a TensorFlow implementation of [**Neural Program Synthesis from Diverse Demonstration Videos**](https://shaohua0116.github.io/demo2program/), which is published in ICML 2018. We provide codes and checkpoints for our model and all baselines presented in the paper. Also, we provide scripts and codes for generating datasets as well as the datasets we used to train and test all models.

This fork is experimenting with the code prediction from planner-generated actions (only for Karel tasks).
Instead of using a sequence of actions obtained by running a program, we observe a more realistic
situation: we try to infer programs from shortest demonstrations to accomplish a task. This
demonstration is generated by running a planner from the initial state to the end state.

The assumption is that such a setting will be more difficult to infer for a neural network
because there are more differences between different tasks as compared to the original setup.
(In the original setup, the differences may stem from different fulfillment of conditions. 
E.g., `while c do a` or `if c do a`. In our setup, the differences will be created by different
plans being optimal for different worlds)

## Results: Karel

There are three categories for which accuracy is measured:
 - Sequence: correct if the generated sequence of commands exactly matches the ground-truth program
 - Program: correct if the program is semantically identical to the ground truth 
(by e.g., unfolding of loops or decomposing if-else statements into two if statements)
 - Execution: correct if the program results in the same execution as the ground truth program
on a set of left-out examples 
(by increasing the number of examples, Execution converges towards Program)

| Methods                        | Number of training steps [10^3] | Execution | Program | Sequence |
| ------------------------------ | :-------: |:-------: | :-----: | :------: |
| Original | 10 |   72.6%   |  49.9%  |  41.0%   |
| Original | 19 |   73.3%   |  50.0%  |  42.0%   |
| Original | 878 |   54.8%   |  35.9%  |  29.9%   |
| Plan | 10 |   33.9%   |  6.4%  |  4.4%   |
| Plan | 19 |   29.6%   |  5.7%  |  4.1%   |
| Plan | 39 |   25.5%   |  3.7%  |  5.3%   |


## Network structure
As interpreting decision making logic in demonstration videos is key to collaborating with and mimicking humans, our goal is to empower machines with this ability. To this end, we propose a **neural program synthesizer** that is able to explicitly synthesize underlying programs from behaviorally diverse and visually complicated demonstration videos, as illustrated in the following figure.

<p align="center">
    <img src="asset/teaser.png" height="256"/>
</p>

We introduce a **summarizer module** as part of our model to improve the network’s ability to integrate multiple demonstrations varying in behavior. We also employ a **multi-task objective** to encourage the model to learn meaningful intermediate representations for end-to-end training. Our proposed model consists three components:
- **Demonstration Encoder** receives a demonstration video as input and produces an embedding that captures an agent’s actions and perception.
- **Summarizer Module** discovers and summarizes where actions diverge between demonstrations and upon which branching conditions subsequent actions are taken.
- **Program Decoder** represents the summarized understanding of demonstrations as a code sequence.

The illustration of the overall architecture is as follows. For more details, please refer to the paper.

<p align="center">
    <img src="asset/model.jpg" height="256"/>
</p>


## Directories
The structure of the repository:
- **./**: training and evaluation scripts
- **./models**: network models used for the experiments
- **./karel_env**: karel environment including dsl, interpreter and dataset generator / loader


## Datasets
- To generate a dataset for Karel environments including programs and demonstrations, use the following script.
```bash
./karel_env/generate_dataset_planner.sh
```


## Usage
### Training
- Train the full model (with the summarizer module and the multi-task objective)
```bath
python trainer.py --model full --dataset_path /path/to/the/dataset/ --dataset_type [karel/vizdoom]
```

- Train the summarizer model (with the summarizer module but without multi-task objective)
```bath
python trainer.py --model summarizer --dataset_path /path/to/the/dataset/ --dataset_type [karel/vizdoom]
```

- Train the baseline program synthesis model (without the summarizer module and multi-task objective)
```bath
python trainer.py --model synthesis_baseline --dataset_path /path/to/the/dataset/ --dataset_type [karel/vizdoom]
```

- Train the baseline program induction model
```bath
python trainer.py --model induction_baseline --dataset_path /path/to/the/dataset/ --dataset_type [karel/vizdoom]
```

- Arguments
    - --debug: set to `True` to see debugging visualization (LSTM masks, etc.)
    - --prefix: a nickname for the training
    - --model: specify which type of models to train/test
    - --dataset\_type: choose between `karel` and `vizdoom`. You can also add your own datasets.
    - --dataset\_path: specify the path to the dataset where you can find a HDF5 file and a .txt file
    - --checkpoint: specify the path to a pre-trained checkpoint
    - Logging
        - --log\_setp: the frequency of outputing log info ([train step  681] Loss: 0.51319 (1.896 sec/batch, 16.878 instances/sec))
        - --write\_summary\_step: the frequency of writing TensorBoard summaries (default 100)
        - --test\_sample\_step: the frequency of performing testing inference during training (default 100)
    - Hyperparameters
        - --num\_k: the number of seen demonstrations (default 10)
        - --batch\_size: the mini-batch size (default 32)
        - --learning\_rate: the learning rate (default 1e-3)
        - --lr\_weight\_decay: set to `True` to perform expotential weight decay on the learning rate
        - --scheduled\_sampling: set to `True` to train models with [scheduled sampling](https://arxiv.org/abs/1506.03099)
    - Architecture
        - --encoder\_rnn\_type: the recurrent model of the demonstration encoder. Choices include RNN, GRU, and LSTM
        - --num\_lstm\_cell\_units: the size of RNN/GRU/LSTM hidden layers (default 512)
        - --demo\_aggregation: how to aggregate the demo features (default average pooling) for synthesis and induction baseline

### Testing
- Evaluate trained models
```bash
python evaler.py --model [full/synthesis_baseline/summarizer/induction_baseline] --dataset_path /path/to/the/dataset/ --dataset_type [karel/vizdoom] [--train_dir /path/to/the/training/dir/ OR --checkpoint /path/to/the/trained/model]
```

## Prerequisites
 - Python packages listed in `requirements.txt`
 - the [Fast Downward planner](https://www.fast-downward.org/) and the environment variable
FASTDOWNWARD pointing to its location

## Comments / Problems
 
 - in all `model_*.py` files, I had to increase the dimension of the embedding map. I wonder if that was 
a mistake in the existing code, or some problem in my understanding (more likely). 
 - **TODO**: understand what is happening here


